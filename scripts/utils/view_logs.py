#!/usr/bin/env python3
"""
æ—¥å¿—æŸ¥çœ‹å·¥å…·
"""

import argparse
import json
import os
from pathlib import Path
from datetime import datetime

import pandas as pd
from prettytable import PrettyTable


def view_task_logs(log_dir: str, task_name: str = None):
    """æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—"""
    log_path = Path(log_dir)
    
    if not log_path.exists():
        print(f"âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}")
        return
    
    # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
    if task_name:
        log_files = list(log_path.glob(f"{task_name}_*.log"))
    else:
        log_files = list(log_path.glob("*.log"))
    
    if not log_files:
        print(f"âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
        return
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶:")
    
    table = PrettyTable(['æ–‡ä»¶å', 'å¤§å°', 'ä¿®æ”¹æ—¶é—´'])
    
    for log_file in sorted(log_files, key=lambda x: x.stat().st_mtime, reverse=True):
        size = log_file.stat().st_size
        mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
        
        table.add_row([
            log_file.name,
            f"{size/1024:.1f}KB" if size < 1024*1024 else f"{size/(1024*1024):.1f}MB",
            mtime.strftime("%Y-%m-%d %H:%M:%S")
        ])
    
    print(table)


def view_batch_details(log_dir: str, task_name: str):
    """æŸ¥çœ‹æ‰¹é‡å¤„ç†è¯¦æƒ…"""
    log_path = Path(log_dir)
    batch_file = log_path / f"{task_name}_batch_details.jsonl"
    
    if not batch_file.exists():
        print(f"âŒ æ‰¹é‡æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {batch_file}")
        return
    
    print(f"ğŸ“Š è¯»å–æ‰¹é‡å¤„ç†è¯¦æƒ…: {batch_file}")
    
    # è¯»å–æ‰¹é‡æ—¥å¿—
    batch_data = []
    with open(batch_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                batch_data.append(json.loads(line))
    
    if not batch_data:
        print("âŒ æ‰¹é‡æ—¥å¿—ä¸ºç©º")
        return
    
    # ç»Ÿè®¡ä¿¡æ¯
    df = pd.DataFrame(batch_data)
    
    print(f"ğŸ“ˆ æ‰¹é‡å¤„ç†ç»Ÿè®¡:")
    print(f"   æ€»è®°å½•æ•°: {len(df):,}")
    print(f"   æ—¶é—´èŒƒå›´: {df['timestamp'].min()} - {df['timestamp'].max()}")
    
    # çŠ¶æ€åˆ†å¸ƒ
    if 'status' in df.columns:
        status_counts = df['status'].value_counts()
        print(f"\nğŸ“Š çŠ¶æ€åˆ†å¸ƒ:")
        for status, count in status_counts.items():
            percentage = count / len(df) * 100
            print(f"   {status}: {count:,} ({percentage:.1f}%)")
    
    # è¯„åˆ†åˆ†å¸ƒ
    if 'score' in df.columns:
        scores = df['score'].dropna()
        if len(scores) > 0:
            print(f"\nâ­ è¯„åˆ†ç»Ÿè®¡:")
            print(f"   å¹³å‡åˆ†: {scores.mean():.2f}")
            print(f"   ä¸­ä½æ•°: {scores.median():.2f}")
            print(f"   åˆ†æ•°èŒƒå›´: {scores.min():.0f} - {scores.max():.0f}")
    
    # æ–‡ä»¶åˆ†å¸ƒ
    if 'file' in df.columns:
        file_counts = df['file'].value_counts()
        print(f"\nğŸ“ æ–‡ä»¶å¤„ç†åˆ†å¸ƒ (Top 10):")
        for file_name, count in file_counts.head(10).items():
            print(f"   {file_name}: {count:,}")


def view_final_stats(log_dir: str, task_name: str):
    """æŸ¥çœ‹æœ€ç»ˆç»Ÿè®¡"""
    log_path = Path(log_dir)
    stats_file = log_path / f"{task_name}_final_stats.json"
    
    if not stats_file.exists():
        print(f"âŒ æœ€ç»ˆç»Ÿè®¡æ–‡ä»¶ä¸å­˜åœ¨: {stats_file}")
        return
    
    with open(stats_file, 'r', encoding='utf-8') as f:
        stats = json.load(f)
    
    print(f"ğŸ¯ æœ€ç»ˆç»Ÿè®¡ - {stats['task_name']}")
    print(f"   å®Œæˆæ—¶é—´: {stats['completion_time']}")
    
    if stats.get('world_size', 1) > 1:
        print(f"   ä½œä¸šé…ç½®: Job {stats['job_index']}/{stats['world_size']}")
    
    stat_data = stats['stats']
    
    print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   æ–‡ä»¶: {stat_data['processed_files']}/{stat_data['total_files']}")
    print(f"   é¡¹ç›®: {stat_data['processed_items']:,}/{stat_data['total_items']:,}")
    print(f"   æˆåŠŸç‡: {stat_data['success_rate']:.1f}%")
    print(f"   å¤„ç†é€Ÿåº¦: {stat_data['processing_speed']:.1f} é¡¹/åˆ†é’Ÿ")
    print(f"   æ€»ç”¨æ—¶: {stat_data['elapsed_time']:.1f} ç§’")


def main():
    parser = argparse.ArgumentParser(description="æŸ¥çœ‹ModelCallä»»åŠ¡æ—¥å¿—")
    parser.add_argument("--log_dir", default="./logs", help="æ—¥å¿—ç›®å½•")
    parser.add_argument("--task", help="ä»»åŠ¡åç§°")
    parser.add_argument("--details", action="store_true", help="æŸ¥çœ‹æ‰¹é‡å¤„ç†è¯¦æƒ…")
    parser.add_argument("--stats", action="store_true", help="æŸ¥çœ‹æœ€ç»ˆç»Ÿè®¡")
    
    args = parser.parse_args()
    
    if args.details:
        if not args.task:
            print("âŒ æŸ¥çœ‹è¯¦æƒ…éœ€è¦æŒ‡å®šä»»åŠ¡åç§° (--task)")
            return
        view_batch_details(args.log_dir, args.task)
    elif args.stats:
        if not args.task:
            print("âŒ æŸ¥çœ‹ç»Ÿè®¡éœ€è¦æŒ‡å®šä»»åŠ¡åç§° (--task)")
            return
        view_final_stats(args.log_dir, args.task)
    else:
        view_task_logs(args.log_dir, args.task)


if __name__ == "__main__":
    main()
