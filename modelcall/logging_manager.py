"""ç»Ÿä¸€æ—¥å¿—ç®¡ç†ç³»ç»Ÿ"""

import os
import sys
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
from contextlib import contextmanager

import json
from tqdm import tqdm


@dataclass
class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    total_files: int = 0
    processed_files: int = 0
    failed_files: int = 0
    total_items: int = 0
    processed_items: int = 0
    success_items: int = 0
    failed_items: int = 0
    start_time: float = field(default_factory=time.time)
    
    @property
    def elapsed_time(self) -> float:
        return time.time() - self.start_time
    
    @property
    def success_rate(self) -> float:
        if self.processed_items == 0:
            return 0.0
        return self.success_items / self.processed_items * 100
    
    @property
    def processing_speed(self) -> float:
        """æ¯åˆ†é’Ÿå¤„ç†çš„é¡¹ç›®æ•°"""
        if self.elapsed_time == 0:
            return 0.0
        return self.processed_items / (self.elapsed_time / 60)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "total_files": self.total_files,
            "processed_files": self.processed_files,
            "failed_files": self.failed_files,
            "total_items": self.total_items,
            "processed_items": self.processed_items,
            "success_items": self.success_items,
            "failed_items": self.failed_items,
            "elapsed_time": self.elapsed_time,
            "success_rate": self.success_rate,
            "processing_speed": self.processing_speed
        }


class ModelCallLogger:
    """ModelCallç»Ÿä¸€æ—¥å¿—ç®¡ç†å™¨"""
    
    def __init__(self, 
                 task_name: str,
                 job_index: int = 0,
                 world_size: int = 1,
                 log_dir: str = "./logs",
                 log_level: str = "INFO"):
        
        self.task_name = task_name
        self.job_index = job_index
        self.world_size = world_size
        self.log_dir = Path(log_dir)
        self.log_level = getattr(logging, log_level.upper())
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if world_size > 1:
            log_filename = f"{task_name}_{timestamp}_job{job_index:03d}.log"
        else:
            log_filename = f"{task_name}_{timestamp}.log"
        
        self.log_file = self.log_dir / log_filename
        
        # è®¾ç½®ç»Ÿè®¡ä¿¡æ¯
        self.stats = ProcessingStats()
        
        # è¿›åº¦æ¡
        self.progress_bars: Dict[str, tqdm] = {}
        
        # æ‰¹é‡æ—¥å¿—ç¼“å­˜
        self.batch_logs: List[Dict[str, Any]] = []
        self.batch_size = 100  # æ¯100æ¡è®°å½•æ‰¹é‡è¾“å‡ºä¸€æ¬¡
        
        # åˆå§‹åŒ–æ—¥å¿—å™¨
        self._setup_logger()
        
        # è®°å½•å¯åŠ¨ä¿¡æ¯
        self.info(f"ğŸš€ ä»»åŠ¡å¯åŠ¨: {task_name}")
        if world_size > 1:
            self.info(f"ğŸŒ åˆ†å¸ƒå¼é…ç½®: Job {job_index}/{world_size}")
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—å™¨"""
        # åˆ›å»ºæ—¥å¿—å™¨
        self.logger = logging.getLogger(f"modelcall.{self.task_name}.{self.job_index}")
        self.logger.setLevel(self.log_level)
        
        # æ¸…é™¤å·²æœ‰çš„handlers
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(self.log_file, encoding='utf-8')
        file_handler.setLevel(self.log_level)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(self.log_level)
        
        # è®¾ç½®æ ¼å¼
        file_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)s | Job%(job_index)d | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        console_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)s | %(message)s',
            datefmt='%H:%M:%S'
        )
        
        file_handler.setFormatter(file_formatter)
        console_handler.setFormatter(console_formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        # æ·»åŠ job_indexåˆ°æ—¥å¿—è®°å½•ä¸­
        class JobIndexFilter(logging.Filter):
            def __init__(self, job_index):
                self.job_index = job_index
            
            def filter(self, record):
                record.job_index = self.job_index
                return True
        
        job_filter = JobIndexFilter(self.job_index)
        file_handler.addFilter(job_filter)
    
    def debug(self, message: str):
        """è°ƒè¯•æ—¥å¿—"""
        self.logger.debug(message)
    
    def info(self, message: str):
        """ä¿¡æ¯æ—¥å¿—"""
        self.logger.info(message)
    
    def warning(self, message: str):
        """è­¦å‘Šæ—¥å¿—"""
        self.logger.warning(message)
    
    def error(self, message: str):
        """é”™è¯¯æ—¥å¿—"""
        self.logger.error(message)
    
    def critical(self, message: str):
        """ä¸¥é‡é”™è¯¯æ—¥å¿—"""
        self.logger.critical(message)
    
    def create_progress_bar(self, name: str, total: int, desc: str = None) -> tqdm:
        """åˆ›å»ºè¿›åº¦æ¡"""
        if desc is None:
            desc = name
        
        progress_bar = tqdm(
            total=total,
            desc=desc,
            unit="items",
            unit_scale=True,
            dynamic_ncols=True,
            position=len(self.progress_bars),
            leave=True
        )
        
        self.progress_bars[name] = progress_bar
        return progress_bar
    
    def update_progress(self, name: str, n: int = 1):
        """æ›´æ–°è¿›åº¦æ¡"""
        if name in self.progress_bars:
            self.progress_bars[name].update(n)
    
    def close_progress_bar(self, name: str):
        """å…³é—­è¿›åº¦æ¡"""
        if name in self.progress_bars:
            self.progress_bars[name].close()
            del self.progress_bars[name]
    
    def log_batch_item(self, item_data: Dict[str, Any]):
        """æ·»åŠ é¡¹ç›®åˆ°æ‰¹é‡æ—¥å¿—ç¼“å­˜"""
        self.batch_logs.append({
            "timestamp": datetime.now().isoformat(),
            "job_index": self.job_index,
            **item_data
        })
        
        # å¦‚æœè¾¾åˆ°æ‰¹é‡å¤§å°ï¼Œè¾“å‡ºæ—¥å¿—
        if len(self.batch_logs) >= self.batch_size:
            self.flush_batch_logs()
    
    def flush_batch_logs(self):
        """è¾“å‡ºæ‰¹é‡æ—¥å¿—"""
        if not self.batch_logs:
            return
        
        # ç»Ÿè®¡æ‰¹é‡ä¿¡æ¯
        batch_stats = {
            "batch_size": len(self.batch_logs),
            "success_count": len([log for log in self.batch_logs if log.get("status") == "success"]),
            "error_count": len([log for log in self.batch_logs if log.get("status") == "error"]),
        }
        
        # è¾“å‡ºæ‰¹é‡ç»Ÿè®¡ä¿¡æ¯
        success_rate = batch_stats["success_count"] / batch_stats["batch_size"] * 100
        self.info(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ: {batch_stats['batch_size']} é¡¹, "
                 f"æˆåŠŸ {batch_stats['success_count']}, "
                 f"å¤±è´¥ {batch_stats['error_count']}, "
                 f"æˆåŠŸç‡ {success_rate:.1f}%")
        
        # ä¿å­˜è¯¦ç»†æ—¥å¿—åˆ°æ–‡ä»¶
        batch_log_file = self.log_dir / f"{self.task_name}_batch_details.jsonl"
        with open(batch_log_file, 'a', encoding='utf-8') as f:
            for log_entry in self.batch_logs:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        
        # æ¸…ç©ºç¼“å­˜
        self.batch_logs.clear()
    
    def update_stats(self, **kwargs):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        for key, value in kwargs.items():
            if hasattr(self.stats, key):
                setattr(self.stats, key, value)
    
    def increment_stats(self, **kwargs):
        """å¢é‡æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        for key, value in kwargs.items():
            if hasattr(self.stats, key):
                current_value = getattr(self.stats, key)
                setattr(self.stats, key, current_value + value)
    
    def log_file_processing(self, filename: str, status: str, 
                           items_processed: int = 0, items_success: int = 0):
        """è®°å½•æ–‡ä»¶å¤„ç†ç»“æœ"""
        self.info(f"ğŸ“ æ–‡ä»¶å¤„ç†å®Œæˆ: {filename}")
        self.info(f"   çŠ¶æ€: {status}")
        if items_processed > 0:
            success_rate = items_success / items_processed * 100
            self.info(f"   å¤„ç†é¡¹ç›®: {items_processed}, æˆåŠŸ: {items_success} ({success_rate:.1f}%)")
        
        # æ›´æ–°ç»Ÿè®¡
        self.increment_stats(processed_files=1)
        if status == "success":
            self.increment_stats(
                processed_items=items_processed,
                success_items=items_success,
                failed_items=items_processed - items_success
            )
        else:
            self.increment_stats(failed_files=1)
    
    def log_periodic_stats(self):
        """å®šæœŸè¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.to_dict()
        
        self.info("=" * 60)
        self.info("ğŸ“Š å½“å‰å¤„ç†ç»Ÿè®¡:")
        self.info(f"   æ–‡ä»¶è¿›åº¦: {stats['processed_files']}/{stats['total_files']}")
        self.info(f"   é¡¹ç›®è¿›åº¦: {stats['processed_items']}/{stats['total_items']}")
        self.info(f"   æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        self.info(f"   å¤„ç†é€Ÿåº¦: {stats['processing_speed']:.1f} é¡¹/åˆ†é’Ÿ")
        self.info(f"   è¿è¡Œæ—¶é—´: {stats['elapsed_time']:.1f} ç§’")
        self.info("=" * 60)
    
    @contextmanager
    def file_processing_context(self, filename: str):
        """æ–‡ä»¶å¤„ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        start_time = time.time()
        self.info(f"ğŸ“‚ å¼€å§‹å¤„ç†æ–‡ä»¶: {filename}")
        
        try:
            yield
            self.info(f"âœ… æ–‡ä»¶å¤„ç†æˆåŠŸ: {filename} (ç”¨æ—¶: {time.time() - start_time:.1f}s)")
        except Exception as e:
            self.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {filename} - {str(e)}")
            raise
    
    def save_final_stats(self):
        """ä¿å­˜æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        stats_file = self.log_dir / f"{self.task_name}_final_stats.json"
        
        final_stats = {
            "task_name": self.task_name,
            "job_index": self.job_index,
            "world_size": self.world_size,
            "completion_time": datetime.now().isoformat(),
            "stats": self.stats.to_dict()
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(final_stats, f, indent=2, ensure_ascii=False)
        
        self.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡å·²ä¿å­˜: {stats_file}")
    
    def finalize(self):
        """å®Œæˆæ—¥å¿—è®°å½•"""
        # è¾“å‡ºå‰©ä½™çš„æ‰¹é‡æ—¥å¿—
        self.flush_batch_logs()
        
        # å…³é—­æ‰€æœ‰è¿›åº¦æ¡
        for name in list(self.progress_bars.keys()):
            self.close_progress_bar(name)
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        self.log_periodic_stats()
        
        # ä¿å­˜æœ€ç»ˆç»Ÿè®¡
        self.save_final_stats()
        
        self.info(f"ğŸ‰ ä»»åŠ¡å®Œæˆ: {self.task_name}")
        self.info(f"ğŸ“„ æ—¥å¿—æ–‡ä»¶: {self.log_file}")


# å…¨å±€æ—¥å¿—ç®¡ç†å™¨å®ä¾‹
_global_logger: Optional[ModelCallLogger] = None


def get_logger() -> Optional[ModelCallLogger]:
    """è·å–å…¨å±€æ—¥å¿—ç®¡ç†å™¨"""
    return _global_logger


def setup_logging(task_name: str, job_index: int = 0, world_size: int = 1, 
                 log_dir: str = "./logs", log_level: str = "INFO") -> ModelCallLogger:
    """è®¾ç½®å…¨å±€æ—¥å¿—ç®¡ç†å™¨"""
    global _global_logger
    _global_logger = ModelCallLogger(
        task_name=task_name,
        job_index=job_index,
        world_size=world_size,
        log_dir=log_dir,
        log_level=log_level
    )
    return _global_logger


def cleanup_logging():
    """æ¸…ç†æ—¥å¿—ç®¡ç†å™¨"""
    global _global_logger
    if _global_logger:
        _global_logger.finalize()
        _global_logger = None
