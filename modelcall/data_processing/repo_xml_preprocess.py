"""ä»£ç ä»“åº“XML/CXMLæ‰“åŒ…æ•ˆæœè¯„åˆ†é¢„å¤„ç†å™¨ (æ”¯æŒRepomixå’ŒRenderGit)"""

import os
import json
import random
import argparse
import copy
import xml.etree.ElementTree as ET
from uuid import uuid4
from pathlib import Path
from typing import List, Dict, Any, Tuple
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import pandas as pd
import numpy as np
import re
from datetime import datetime

from ..utils import get_tos_config, get_filesystem, process_text, save_progress_stat
from .base import BasePreprocessor


DEFAULT_FILE_STAT = {
    "prompt_conf": "",
    "model_conf": "",
    "rating_times": 0,
    "voting_status": False,
    "raw_file_path": "",
    "formatted_file_path": "",
    "taged_file_paths": [],
    "voting_file_path": "",
    "n_sample": 0,
}


class RepoXMLPreprocessor(BasePreprocessor):
    """ä»£ç ä»“åº“XML/CXMLæ–‡ä»¶é¢„å¤„ç†å™¨ (æ”¯æŒRepomixå’ŒRenderGit)"""
    
    def __init__(self, raw_path: str, output_dir: str, stat_dir: str, 
                 fs_cfg: Dict[str, Any], max_tokens: int = 32768, 
                 num_proc: int = 32, seed: int = 42, num_files: int = -1,
                 languages: List[str] = None, batch_size: int = 1000):
        
        super().__init__(raw_path, output_dir, stat_dir, fs_cfg, max_tokens, num_proc, batch_size)
        
        self.seed = seed
        self.num_files = num_files
        self.languages = languages or []  # å¦‚æœæŒ‡å®šï¼Œåªå¤„ç†è¿™äº›è¯­è¨€
        
        # è®¾ç½®éšæœºç§å­
        random.seed(seed)
        np.random.seed(seed)
        
        print(f"RepoXMLPreprocessor initialized:")
        print(f"  Raw path: {raw_path}")
        print(f"  Output dir: {output_dir}")
        print(f"  Stat dir: {stat_dir}")
        print(f"  Max tokens: {max_tokens}")
        print(f"  Num processes: {num_proc}")
        if self.languages:
            print(f"  Target languages: {self.languages}")
    
    def get_file_list(self) -> Dict[str, List[Tuple[str, str, str]]]:
        """è·å–è¦å¤„ç†çš„XML/CXMLæ–‡ä»¶åˆ—è¡¨ï¼ŒæŒ‰è¯­è¨€åˆ†ç»„"""
        
        base_path = Path(self.raw_path)
        
        # è·å–æ‰€æœ‰è¯­è¨€ç›®å½•
        if self.languages:
            # åªå¤„ç†æŒ‡å®šè¯­è¨€
            language_dirs = [base_path / lang for lang in self.languages if (base_path / lang).exists()]
        else:
            # å¤„ç†æ‰€æœ‰è¯­è¨€
            language_dirs = [d for d in base_path.iterdir() if d.is_dir()]
        
        print(f"Found {len(language_dirs)} language directories")
        
        # æŒ‰è¯­è¨€æ”¶é›†æ–‡ä»¶
        files_by_language = {}
        total_files = 0
        
        for lang_dir in language_dirs:
            language = lang_dir.name
            # æ”¯æŒXMLå’ŒCXMLæ ¼å¼
            xml_files = list(lang_dir.glob("*.xml")) + list(lang_dir.glob("*.cxml"))
            
            if xml_files:
                files_by_language[language] = []
                for xml_file in xml_files:
                    files_by_language[language].append(str(xml_file))
                    total_files += 1
        
        print(f"Found {total_files} total XML/CXML files across {len(files_by_language)} languages")
        
        # é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡ï¼ˆæŒ‰è¯­è¨€å¹³å‡åˆ†é…ï¼‰
        if self.num_files > 0 and len(files_by_language) > 0:
            files_per_lang = max(1, self.num_files // len(files_by_language))
            for language in files_by_language:
                files_by_language[language] = files_by_language[language][:files_per_lang]
            
            total_limited = sum(len(files) for files in files_by_language.values())
            print(f"Limited to {total_limited} files for processing (~{files_per_lang} per language)")
        
        return files_by_language
    
    def extract_xml_content(self, xml_path: str) -> Dict[str, Any]:
        """æå–XMLæ–‡ä»¶çš„å…³é”®ä¿¡æ¯"""
        try:
            with open(xml_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–åŸºæœ¬ä¿¡æ¯
            repo_info = self.extract_repo_info(content)
            
            # æå–æ–‡ä»¶ç»“æ„ä¿¡æ¯
            structure_info = self.extract_structure_info(content)
            
            # æå–ä»£ç å†…å®¹
            code_content = self.extract_code_content(content)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = self.calculate_stats(content, code_content)
            
            return {
                "repo_info": repo_info,
                "structure_info": structure_info,
                "code_content": code_content,
                "stats": stats,
                "raw_content": content  # ä¿ç•™åŸå§‹å†…å®¹ç”¨äºè¯„åˆ†
            }
            
        except Exception as e:
            print(f"Error extracting XML content from {xml_path}: {e}")
            return None
    
    def extract_repo_info(self, content: str) -> Dict[str, str]:
        """æå–ä»“åº“åŸºæœ¬ä¿¡æ¯"""
        # ä»æ–‡ä»¶åæå–ç”¨æˆ·/ä»“åº“ä¿¡æ¯
        repo_pattern = r'([^_]+)_([^_]+)_([a-f0-9]+)\.xml'
        
        info = {
            "user": "",
            "repo": "",
            "commit_hash": "",
            "full_name": ""
        }
        
        # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥ä»XMLå†…å®¹ä¸­æå–æ›´å¤šä¿¡æ¯
        return info
    
    def extract_structure_info(self, content: str) -> Dict[str, Any]:
        """æå–ç»“æ„ä¿¡æ¯ï¼ˆåŸºäºå®é™…æ–‡ä»¶å†…å®¹ï¼‰"""
        # ä¸å†ä»directory_structureæå–ï¼Œè€Œæ˜¯åŸºäºå®é™…çš„<files>éƒ¨åˆ†
        # è·å–<files>éƒ¨åˆ†çš„æ–‡ä»¶è·¯å¾„æ¥æ¨æ–­ç»“æ„ä¿¡æ¯
        files_section = re.search(r'<files>(.*?)</files>', content, re.DOTALL)
        
        if files_section:
            files_content = files_section.group(1)
            file_entries = re.findall(r'<file path="([^"]+)">', files_content)
            
            return {
                "files": file_entries,
                "total_files": len(file_entries),  # åŸºäºå®é™…æ–‡ä»¶æ•°
                "has_readme": any('readme' in f.lower() for f in file_entries),
                "has_license": any('license' in f.lower() for f in file_entries),
                "has_gitignore": '.gitignore' in file_entries
            }
        
        return {"files": [], "total_files": 0}
    
    def extract_code_content(self, content: str) -> Dict[str, Any]:
        """æå–ä»£ç å†…å®¹"""
        # æå–<files>éƒ¨åˆ†
        files_section = re.search(r'<files>(.*?)</files>', content, re.DOTALL)
        
        if files_section:
            files_content = files_section.group(1)
            
            # è®¡ç®—ä»£ç è¡Œæ•°å’Œæ–‡ä»¶æ•°
            file_entries = re.findall(r'<file path="([^"]+)">(.*?)</file>', 
                                    files_content, re.DOTALL)
            
            total_lines = 0
            file_types = {}
            
            for file_path, file_content in file_entries:
                lines = len(file_content.split('\n'))
                total_lines += lines
                
                ext = os.path.splitext(file_path)[1].lower()
                file_types[ext] = file_types.get(ext, 0) + 1
            
            return {
                "total_files_with_content": len(file_entries),
                "total_lines": total_lines,
                "file_types": file_types,
                "avg_lines_per_file": total_lines / len(file_entries) if file_entries else 0
            }
        
        return {"total_files_with_content": 0, "total_lines": 0}
    
    def calculate_stats(self, content: str, code_info: Dict) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        content_length = len(content)
        content_lines = len(content.split('\n'))
        
        return {
            "content_length": content_length,
            "content_lines": content_lines,
            "avg_line_length": content_length / max(content_lines, 1),
        }
    
    def process_language_files(self, language: str, xml_files: List[str]) -> Tuple[bool, int]:
        """å¤„ç†ä¸€ä¸ªè¯­è¨€çš„æ‰€æœ‰XMLæ–‡ä»¶ï¼Œåˆå¹¶ä¸ºä¸€ä¸ªParquetï¼Œæ”¯æŒåˆ†æ‰¹å†™å…¥å’Œæ–­ç‚¹ç»­ä¼ """
        try:
            print(f"Processing {len(xml_files)} {language} files...")
            
            output_path = os.path.join(self.output_dir, f"{language}.parquet")
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # åˆ›å»ºè¿›åº¦çŠ¶æ€æ–‡ä»¶è·¯å¾„
            progress_file = os.path.join(self.stat_dir, f"{language}_progress.json")
            
            # æ£€æŸ¥æ–­ç‚¹ç»­ä¼ çŠ¶æ€
            processed_files = set()
            all_items = []
            
            if os.path.exists(output_path) and os.path.exists(progress_file):
                try:
                    # è¯»å–å·²å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
                    with open(progress_file, 'r', encoding='utf-8') as f:
                        progress_data = json.load(f)
                    processed_files = set(progress_data.get('processed_files', []))
                    
                    # è¯»å–å·²æœ‰çš„ç»“æœ
                    existing_df = pd.read_parquet(output_path)
                    all_items = existing_df.to_dict('records')
                    
                    print(f"ğŸ“Š æ–­ç‚¹ç»­ä¼ : å·²å¤„ç† {len(processed_files)} ä¸ªæ–‡ä»¶, åŒ…å« {len(all_items)} æ¡è®°å½•")
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è¯»å–æ–­ç‚¹ç»­ä¼ æ•°æ®: {e}, ä»å¤´å¼€å§‹å¤„ç†")
                    processed_files = set()
                    all_items = []
            
            # è¿‡æ»¤æ‰å·²å¤„ç†çš„æ–‡ä»¶
            remaining_files = [f for f in xml_files if f not in processed_files]
            print(f"ğŸ“ éœ€è¦å¤„ç† {len(remaining_files)} ä¸ªæ–°æ–‡ä»¶")
            
            if not remaining_files:
                print(f"âœ… {language} è¯­è¨€æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ")
                return True, len(all_items)
            
            # ç´¯ç§¯æ¨¡å¼å¤„ç†æ–‡ä»¶ï¼ˆä¸æ˜¯åˆ†æ‰¹ï¼Œè€Œæ˜¯ç´¯ç§¯å¼ä¿å­˜ï¼‰
            items_since_last_save = 0
            
            for i, xml_file in enumerate(tqdm(remaining_files, desc=f"Processing {language}", unit="files")):
                try:
                    # æå–XMLå†…å®¹
                    extracted_data = self.extract_xml_content(xml_file)
                    if not extracted_data:
                        continue
                    
                    # ä»æ–‡ä»¶åæå–ä»“åº“ä¿¡æ¯ (æ”¯æŒXMLå’ŒCXMLæ ¼å¼)
                    filename = os.path.basename(xml_file)
                    repo_match = re.match(r'([^_]+)_([^_]+)_([a-f0-9]+)\.(xml|cxml)', filename)
                    
                    if repo_match:
                        user, repo, commit_hash, file_ext = repo_match.groups()
                        extracted_data["repo_info"].update({
                            "user": user,
                            "repo": repo,
                            "commit_hash": commit_hash,
                            "full_name": f"{user}/{repo}",
                            "packaging_tool": "repomix" if file_ext == "xml" else "rendergit"
                        })
                    
                    # æ„å»ºæ ‡å‡†æ ¼å¼æ•°æ®
                    item = {
                        "id": str(uuid4()),
                        "text": extracted_data["raw_content"],  # å®Œæ•´çš„XMLå†…å®¹ä½œä¸ºè¯„åˆ†å¯¹è±¡
                        "source": "repo_xml",
                        
                        # ä»“åº“ä¿¡æ¯
                        "repo_user": extracted_data["repo_info"]["user"],
                        "repo_name": extracted_data["repo_info"]["repo"],
                        "repo_full_name": extracted_data["repo_info"]["full_name"],
                        "commit_hash": extracted_data["repo_info"]["commit_hash"],
                        "language": language,
                        "packaging_tool": extracted_data["repo_info"].get("packaging_tool", "unknown"),
                        
                        # ç»“æ„ä¿¡æ¯ï¼ˆåŸºäºå®é™…æ–‡ä»¶ï¼‰
                        "files_with_content": extracted_data["code_content"]["total_files_with_content"],
                        "total_lines": extracted_data["code_content"]["total_lines"],
                        "avg_lines_per_file": extracted_data["code_content"]["avg_lines_per_file"],
                        
                        # è´¨é‡æŒ‡æ ‡ï¼ˆåŸºäºå®é™…æ–‡ä»¶ï¼‰
                        "has_readme": extracted_data["structure_info"]["has_readme"],
                        "has_license": extracted_data["structure_info"]["has_license"],
                        "has_gitignore": extracted_data["structure_info"]["has_gitignore"],
                        
                        # å†…å®¹ç»Ÿè®¡
                        "content_length": extracted_data["stats"]["content_length"],
                        "content_lines": extracted_data["stats"]["content_lines"],
                        "avg_line_length": extracted_data["stats"]["avg_line_length"],
                        
                        # æ–‡ä»¶ç±»å‹åˆ†å¸ƒ
                        "file_types": json.dumps(extracted_data["code_content"]["file_types"]),
                        
                        # åŸå§‹æ–‡ä»¶è·¯å¾„
                        "xml_file_path": xml_file,
                        "file_extension": filename.split('.')[-1]  # xml æˆ– cxml
                    }
                    
                    # åˆ›å»ºæˆªæ–­ç‰ˆæœ¬ç”¨äºAPIè°ƒç”¨
                    truncated_text = process_text(item["text"], self.enc, self.max_tokens)
                    item[f'content_truncate_{self.max_tokens//1024}k'] = truncated_text
                    
                    all_items.append(item)
                    processed_files.add(xml_file)
                    items_since_last_save += 1
                    
                    # æŒ‰æ‰¹æ¬¡ä¿å­˜è¿›åº¦ï¼ˆé¿å…é¢‘ç¹IOï¼‰
                    if items_since_last_save >= self.batch_size or i == len(remaining_files) - 1:
                        # ä¿å­˜æ•°æ®æ–‡ä»¶
                        df = pd.DataFrame(all_items)
                        df.to_parquet(output_path, engine='pyarrow')
                        
                        # æ›´æ–°è¿›åº¦æ–‡ä»¶
                        progress_data = {
                            "language": language,
                            "processed_files": list(processed_files),
                            "total_items": len(all_items),
                            "last_update": datetime.now().isoformat(),
                            "batch_size": self.batch_size
                        }
                        
                        with open(progress_file, 'w', encoding='utf-8') as f:
                            json.dump(progress_data, f, indent=2, ensure_ascii=False)
                        
                        print(f"ğŸ’¾ è¿›åº¦ä¿å­˜: {len(all_items)} æ¡è®°å½•, å¤„ç†äº† {len(processed_files)} ä¸ªæ–‡ä»¶")
                        items_since_last_save = 0
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {os.path.basename(xml_file)}: {e}")
                    continue
            
            if all_items:
                print(f"âœ… {language} å¤„ç†å®Œæˆ: {len(all_items)} æ¡è®°å½•")
                return True, len(all_items)
            else:
                print(f"âŒ {language} æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                return False, 0
            
        except Exception as e:
            print(f"âŒ å¤„ç† {language} è¯­è¨€å¤±è´¥: {e}")
            return False, 0
    
    def run(self):
        """è¿è¡Œé¢„å¤„ç†"""
        print("=== Repomix XML Preprocessing ===")
        
        # ä¿å­˜é…ç½®
        os.makedirs(self.stat_dir, exist_ok=True)
        config_path = os.path.join(self.stat_dir, "preprocess_config.json")
        config_to_save = {
            "raw_path": self.raw_path,
            "output_dir": self.output_dir,
            "stat_dir": self.stat_dir,
            "max_tokens": self.max_tokens,
            "num_proc": self.num_proc,
            "seed": self.seed,
            "num_files": self.num_files,
            "languages": self.languages
        }
        
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(config_to_save, f, indent=2, ensure_ascii=False)
        
        # è·å–æŒ‰è¯­è¨€åˆ†ç»„çš„æ–‡ä»¶
        files_by_language = self.get_file_list()
        
        if not files_by_language:
            print("No files to process!")
            return
        
        total_languages = len(files_by_language)
        total_files = sum(len(files) for files in files_by_language.values())
        print(f"Processing {total_files} files across {total_languages} languages")
        
        # æŒ‰è¯­è¨€å¤„ç†ï¼ˆé¡ºåºå¤„ç†ï¼Œé¿å…å†…å­˜å‹åŠ›ï¼‰
        n_success_languages = 0
        n_fail_languages = 0
        total_items = 0
        
        for language, xml_files in files_by_language.items():
            print(f"\n=== Processing {language} ({len(xml_files)} files) ===")
            
            success, n_items = self.process_language_files(language, xml_files)
            
            if success:
                n_success_languages += 1
                total_items += n_items
                
                # ä¿å­˜å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯
                from datetime import datetime
                stat = copy.deepcopy(DEFAULT_FILE_STAT)
                stat["raw_file_path"] = f"{language}_combined"
                stat["formatted_file_path"] = os.path.join(self.output_dir, f"{language}.parquet")
                stat["n_sample"] = n_items
                stat["processing_complete"] = True
                stat["language"] = language
                stat["num_xml_files"] = len(xml_files)
                stat["batch_size"] = self.batch_size
                stat["max_tokens"] = self.max_tokens
                stat["processing_time"] = datetime.now().isoformat()
                
                # æ·»åŠ è¾“å‡ºæ–‡ä»¶æ£€æŸ¥
                output_file = os.path.join(self.output_dir, f"{language}.parquet")
                if os.path.exists(output_file):
                    stat["output_file_exists"] = True
                    stat["output_file_size_bytes"] = os.path.getsize(output_file)
                else:
                    stat["output_file_exists"] = False
                
                stat_file = os.path.join(self.stat_dir, f"{language}_combined.json")
                try:
                    save_progress_stat(stat_file, stat)
                    print(f"ğŸ“Š Saved statistics for {language}: {n_items} items")
                except Exception as e:
                    print(f"Failed to write stat file {stat_file}: {e}")
            else:
                n_fail_languages += 1
        
        print(f"\n=== Processing completed ===")
        print(f"Languages processed: {n_success_languages}/{total_languages}")
        print(f"Total items: {total_items}")
        success_rate = n_success_languages / total_languages * 100 if total_languages > 0 else 0
        print(f"Success rate: {success_rate:.2f}%")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description="Preprocess repository XML/CXML files for scoring (Repomix and RenderGit)")
    
    parser.add_argument("--raw_path", type=str, required=True,
                       help="Path to repomix_output or rendergit_output directory")
    parser.add_argument("--output_dir", type=str, required=True,
                       help="Directory to save the processed dataset")
    parser.add_argument("--stat_dir", type=str, required=True,
                       help="Directory to save processing statistics")
    parser.add_argument("--num_proc", type=int, default=16,
                       help="Number of processes to use for preprocessing")
    parser.add_argument("--max_tokens", type=int, default=32768,
                       help="Maximum number of tokens per example")
    parser.add_argument("--num_files", type=int, default=-1,
                       help="Number of files to process (-1 for all)")
    parser.add_argument("--seed", type=int, default=42,
                       help="Random seed for sampling")
    parser.add_argument("--languages", type=str, nargs="+", default=None,
                       help="Specific languages to process (e.g., Python Java)")
    
    args = parser.parse_args()
    
    # è·å–æ–‡ä»¶ç³»ç»Ÿé…ç½®ï¼ˆè¿™é‡Œä¸»è¦æ˜¯æœ¬åœ°å¤„ç†ï¼‰
    fs_cfg = {"local": {}}
    
    # åˆ›å»ºå¹¶è¿è¡Œé¢„å¤„ç†å™¨
    preprocessor = RepoXMLPreprocessor(
        raw_path=args.raw_path,
        output_dir=args.output_dir,
        stat_dir=args.stat_dir,
        fs_cfg=fs_cfg,
        max_tokens=args.max_tokens,
        num_proc=args.num_proc,
        seed=args.seed,
        num_files=args.num_files,
        languages=args.languages
    )
    
    preprocessor.run()


if __name__ == "__main__":
    main()
