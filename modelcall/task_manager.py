"""ä»»åŠ¡ç®¡ç†å™¨ - ç»Ÿä¸€ä»»åŠ¡é…ç½®å’Œæ‰§è¡Œ"""

from __future__ import annotations

import os
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

import yaml
from easydict import EasyDict

from .pipeline.concurrent_processor import ConcurrentFileProcessor
from .utils import get_tos_config
from .logging_manager import setup_logging, cleanup_logging, get_logger
from .data_processing.universal_preprocessor import create_preprocessor_from_config
from .data_processing.github_raw_code_preprocess import GitHubRawCodePreprocessor
from .data_processing.repo_xml_preprocess import RepoXMLPreprocessor


class TaskManager:
    """ä»»åŠ¡ç®¡ç†å™¨"""
    
    def __init__(self, task_config_path: str):
        self.task_config_path = task_config_path
        self.config = self._load_task_config()
        self.fs_cfg = self._get_filesystem_config()
    
    def _load_task_config(self) -> EasyDict:
        """åŠ è½½ä»»åŠ¡é…ç½®"""
        with open(self.task_config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return EasyDict(config)
    
    def _get_filesystem_config(self) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶ç³»ç»Ÿé…ç½®"""
        ak, sk, endpoint, region = get_tos_config()
        return {"tos": {"ak": ak, "sk": sk, "endpoint": endpoint, "region": region}}
    
    def _resolve_paths(self) -> Dict[str, str]:
        """è§£æé…ç½®ä¸­çš„è·¯å¾„ï¼Œæ”¯æŒå˜é‡æ›¿æ¢"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è·¯å¾„å˜é‡æ›¿æ¢
        replacements = {
            "{timestamp}": timestamp,
            "{task_name}": self.config.task_name
        }
        
        paths = {}
        
        # å¤„ç†è¾“å…¥è¾“å‡ºè·¯å¾„
        input_folder = self.config.data.input_folder
        output_folder = self.config.data.output_folder
        stat_folder = self.config.data.stat_folder
        
        for key, value in replacements.items():
            output_folder = output_folder.replace(key, value)
            stat_folder = stat_folder.replace(key, value)
        
        # æ·»åŠ TOSå‰ç¼€ï¼ˆå¦‚æœéœ€è¦ï¼‰- åªå¯¹ç›¸å¯¹è·¯å¾„ï¼ˆä¸ä»¥tos://ã€/ã€.å¼€å¤´ï¼‰
        if not input_folder.startswith(("tos://", "/", "./")):
            input_folder = f"tos://agi-data/{input_folder}"
        if not output_folder.startswith(("tos://", "/", "./")):
            output_folder = f"tos://agi-data/{output_folder}"
        
        paths = {
            "input_folder": input_folder,
            "output_folder": output_folder,
            "stat_folder": stat_folder,
            "model_config_path": self.config.model.config_path,
            "prompt_config_path": self.config.prompt.config_path
        }
        
        return paths
    
    def _setup_environment(self) -> None:
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        env_config = self.config.get("environment", {})
        
        # è·å–APIç¯å¢ƒé…ç½®æ–‡ä»¶è·¯å¾„
        api_env_file = env_config.get("config_path")
        
        if not api_env_file:
            print("âš ï¸ æœªæŒ‡å®šAPIç¯å¢ƒé…ç½®æ–‡ä»¶")
            return
        
        # åŠ è½½APIç¯å¢ƒæ–‡ä»¶
        if os.path.exists(api_env_file):
            print(f"ğŸ”§ åŠ è½½APIç¯å¢ƒé…ç½®: {api_env_file}")
            self._load_env_file(api_env_file)
            
            print(f"âœ… APIç¯å¢ƒå˜é‡å·²åŠ è½½: BASE_URL={os.environ.get('BASE_URL', 'Not set')}")
            print(f"âœ… APIç¯å¢ƒå˜é‡å·²åŠ è½½: API_KEY={os.environ.get('API_KEY', 'Not set')}")
        else:
            print(f"âŒ APIç¯å¢ƒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {api_env_file}")
        
        # è·å–TOSç¯å¢ƒé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        tos_env_file = env_config.get("tos_config_path")
        if tos_env_file and os.path.exists(tos_env_file):
            print(f"ğŸ”§ åŠ è½½TOSç¯å¢ƒé…ç½®: {tos_env_file}")
            self._load_env_file(tos_env_file)
            print(f"âœ… TOSç¯å¢ƒå˜é‡å·²åŠ è½½: TOS_ENDPOINT={os.environ.get('TOS_ENDPOINT', 'Not set')}")
        
        # è®¾ç½®è¶…æ—¶
        if "timeout" in env_config:
            os.environ["REQUEST_TIMEOUT"] = str(env_config["timeout"])
    
    def _load_env_file(self, env_file: str) -> None:
        """åŠ è½½ç¯å¢ƒé…ç½®æ–‡ä»¶"""
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    # å¤„ç† export VAR=value æ ¼å¼
                    if line.startswith('export '):
                        line = line[7:]  # ç§»é™¤ 'export '
                    
                    # åˆ†å‰²å˜é‡åå’Œå€¼
                    key, value = line.split('=', 1)
                    # ç§»é™¤å¼•å·
                    value = value.strip('"\'')
                    os.environ[key] = value
    
    def create_processor(self, job_index: int = 0, world_size: int = 1, run_index: int = 1) -> ConcurrentFileProcessor:
        """åˆ›å»ºå¤„ç†å™¨å®ä¾‹"""
        paths = self._resolve_paths()
        
        # å¦‚æœæ˜¯å¤šè½®è¿è¡Œï¼Œè°ƒæ•´è¾“å‡ºè·¯å¾„
        if self.config.distributed.get("num_runs", 1) > 1:
            output_folder = paths["output_folder"].replace("{run_index}", str(run_index))
            paths["output_folder"] = output_folder
        
        # åˆ›å»ºå¤„ç†å™¨
        processor = ConcurrentFileProcessor(
            input_folder=paths["input_folder"],
            output_folder=paths["output_folder"],
            stat_folder=paths["stat_folder"],
            model_config_path=paths["model_config_path"],
            prompt_config_path=paths["prompt_config_path"],
            fs_cfg=self.fs_cfg,
            max_concurrent_files=self.config.concurrency.max_concurrent_files,
            max_concurrent_requests=self.config.concurrency.max_concurrent_requests,
            chunk_size=self.config.concurrency.chunk_size,
            parquet_save_interval=self.config.concurrency.parquet_save_interval,
            input_key=self.config.data.input_key,
            prompt_format_key=self.config.data.prompt_format_key,
            enable_format_validation_retry=self.config.retry.enable_format_validation_retry
        )
        
        return processor
    
    async def run_preprocess(self, job_index: int = 0, world_size: int = None) -> None:
        """è¿è¡Œé¢„å¤„ç†ä»»åŠ¡"""
        if world_size is None:
            world_size = self.config.distributed.get("world_size", 1)
        
        preprocess_config = self.config.get("preprocess")
        if not preprocess_config:
            return
        
        logger = get_logger()
        if logger:
            logger.info("ğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        # è§£æé¢„å¤„ç†è·¯å¾„
        paths = self._resolve_paths()
        preprocess_input = preprocess_config.get("input_folder", paths["input_folder"])
        preprocess_output = preprocess_config.get("output_folder", paths["input_folder"] + "_preprocessed")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰è„šæœ¬
        script_type = preprocess_config.get("script_type", "universal")
        
        if script_type == "github_raw_code":
            # ä½¿ç”¨GitHubåŸå§‹ä»£ç é¢„å¤„ç†è„šæœ¬
            if logger:
                logger.info("ğŸ”§ ä½¿ç”¨GitHubåŸå§‹ä»£ç é¢„å¤„ç†è„šæœ¬")
            
            # å¤„ç†è°ƒè¯•æ¨¡å¼çš„æ–‡ä»¶é™åˆ¶
            debug_max_files = None
            if self.config.debug.enabled and hasattr(self.config.debug, 'max_files'):
                debug_max_files = self.config.debug.max_files
            
            num_files = debug_max_files if debug_max_files is not None else preprocess_config.get("num_files", -1)
            
            preprocessor = GitHubRawCodePreprocessor(
                raw_path=preprocess_input,
                output_dir=preprocess_output.replace("tos://agi-data/", ""),  # ç§»é™¤å‰ç¼€
                stat_dir=paths["stat_folder"] + "_preprocess",
                fs_cfg=self.fs_cfg,
                max_tokens=preprocess_config.get("max_tokens", 32768),
                num_proc=preprocess_config.get("num_proc", 32),
                seed=preprocess_config.get("seed", 42),
                num_files=num_files
            )
            
            # è¿è¡Œé¢„å¤„ç†
            preprocessor.run()
            
        elif script_type == "repo_xml":
            # ä½¿ç”¨ä»£ç ä»“åº“XML/CXMLé¢„å¤„ç†è„šæœ¬
            if logger:
                logger.info("ğŸ”§ ä½¿ç”¨ä»£ç ä»“åº“XML/CXMLé¢„å¤„ç†è„šæœ¬")
            
            # å¤„ç†è°ƒè¯•æ¨¡å¼çš„æ–‡ä»¶é™åˆ¶
            debug_max_files = None
            if self.config.debug.enabled and hasattr(self.config.debug, 'max_files'):
                debug_max_files = self.config.debug.max_files
            
            num_files = debug_max_files if debug_max_files is not None else preprocess_config.get("num_files", -1)
            
            preprocessor = RepoXMLPreprocessor(
                raw_path=preprocess_input,
                output_dir=preprocess_output.replace("tos://agi-data/", ""),  # ç§»é™¤å‰ç¼€
                stat_dir=paths["stat_folder"] + "_preprocess",
                fs_cfg=self.fs_cfg,
                max_tokens=preprocess_config.get("max_tokens", 32768),
                num_proc=preprocess_config.get("num_proc", 16),
                seed=preprocess_config.get("seed", 42),
                num_files=num_files,
                languages=preprocess_config.get("languages")
            )
            
            # è¿è¡Œé¢„å¤„ç†
            preprocessor.run()
            
        else:
            # ä½¿ç”¨é€šç”¨é¢„å¤„ç†å™¨
            if logger:
                logger.info("ğŸ”§ ä½¿ç”¨é€šç”¨é¢„å¤„ç†å™¨")
            
            # æ·»åŠ TOSå‰ç¼€
            if not preprocess_input.startswith(("tos://", "/", ".")):
                preprocess_input = f"tos://agi-data/{preprocess_input}"
            if not preprocess_output.startswith(("tos://", "/", ".")):
                preprocess_output = f"tos://agi-data/{preprocess_output}"
            
            # åˆ›å»ºé¢„å¤„ç†å™¨
            preprocessor = create_preprocessor_from_config(
                preprocess_config=preprocess_config,
                raw_path=preprocess_input,
                output_dir=preprocess_output,
                stat_dir=paths["stat_folder"] + "_preprocess",
                fs_cfg=self.fs_cfg,
                max_tokens=preprocess_config.get("max_tokens", 32768),
                num_proc=preprocess_config.get("num_proc", 32)
            )
            
            # è¿è¡Œé¢„å¤„ç†
            preprocessor.run()
        
        if logger:
            logger.info("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        
        # æ›´æ–°ä»»åŠ¡é…ç½®ä¸­çš„è¾“å…¥è·¯å¾„ä¸ºé¢„å¤„ç†åçš„è·¯å¾„
        # ä¿æŒé¢„å¤„ç†è¾“å‡ºè·¯å¾„çš„åŸå§‹æ ¼å¼ï¼ˆæœ¬åœ°/TOSï¼‰
        self.config.data.input_folder = preprocess_output

    async def run_task(self, job_index: int = 0, world_size: int = None) -> None:
        """è¿è¡Œä»»åŠ¡ï¼ˆåŒ…æ‹¬å¯é€‰çš„é¢„å¤„ç†ï¼‰"""
        # ä½¿ç”¨é…ç½®ä¸­çš„world_sizeï¼Œé™¤éæ˜ç¡®æŒ‡å®š
        if world_size is None:
            world_size = self.config.distributed.get("world_size", 1)
        
        # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
        logging_config = self.config.get("logging", {})
        logger = setup_logging(
            task_name=self.config.task_name,
            job_index=job_index,
            world_size=world_size,
            log_level=logging_config.get("level", "INFO")
        )
        
        # è®¾ç½®æ‰¹é‡æ—¥å¿—å¤§å°
        if hasattr(logger, 'batch_size'):
            logger.batch_size = logging_config.get("batch_size", 100)
        
        try:
            self._setup_environment()
            
            logger.info(f"ğŸ“‹ ä»»åŠ¡æè¿°: {self.config.description}")
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨åˆ†å¸ƒå¼
            if self.config.distributed.get("enabled", False) and world_size > 1:
                logger.info(f"ğŸ”€ åˆ†å¸ƒå¼æ¨¡å¼å·²å¯ç”¨")
            
            # è¿è¡Œé¢„å¤„ç†ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if self.config.get("preprocess") and self.config.preprocess.get("enabled", False):
                await self.run_preprocess(job_index, world_size)
            
            # å¤šè½®è¿è¡Œæ”¯æŒ
            num_runs = self.config.distributed.get("num_runs", 1)
            
            for run_index in range(1, num_runs + 1):
                if num_runs > 1:
                    logger.info(f"ğŸ¯ === ç¬¬ {run_index}/{num_runs} è½®è¿è¡Œ ===")
                
                # åˆ›å»ºå¤„ç†å™¨
                processor = self.create_processor(job_index, world_size, run_index)
                
                # è·å–è¦å¤„ç†çš„æ–‡ä»¶
                debug_files = self.config.debug.max_files if self.config.debug.enabled else None
                files = processor.get_files_to_process(
                    debug_files=debug_files,
                    job_index=job_index,
                    world_size=world_size
                )
                
                if not files:
                    logger.warning(f"æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„æ–‡ä»¶ (Job {job_index}/{world_size})")
                    continue
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                logger.update_stats(total_files=len(files))
                logger.info(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†")
                
                # è¿è¡Œå¤„ç†
                debug_items = self.config.debug.max_items_per_file if self.config.debug.enabled else None
                await processor.process_files(
                    files=files,
                    resume=self.config.options.resume,
                    debug_items=debug_items,
                    delete_existing=self.config.options.delete_existing
                )
                
                logger.info(f"âœ… ç¬¬ {run_index} è½®è¿è¡Œå®Œæˆ")
            
            logger.info(f"ğŸ‰ ä»»åŠ¡ {self.config.task_name} æ‰§è¡Œå®Œæˆ!")
            
        finally:
            # æ¸…ç†æ—¥å¿—ç³»ç»Ÿ
            cleanup_logging()
    
    def print_config_summary(self) -> None:
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print(f"\nğŸ“‹ ä»»åŠ¡é…ç½®æ‘˜è¦:")
        print(f"   ä»»åŠ¡åç§°: {self.config.task_name}")
        print(f"   ä»»åŠ¡æè¿°: {self.config.description}")
        
        paths = self._resolve_paths()
        print(f"   è¾“å…¥è·¯å¾„: {paths['input_folder']}")
        print(f"   è¾“å‡ºè·¯å¾„: {paths['output_folder']}")
        print(f"   ç»Ÿè®¡è·¯å¾„: {paths['stat_folder']}")
        
        print(f"   å¹¶å‘æ–‡ä»¶: {self.config.concurrency.max_concurrent_files}")
        print(f"   å¹¶å‘è¯·æ±‚: {self.config.concurrency.max_concurrent_requests}")
        
        if self.config.distributed.enabled:
            print(f"   åˆ†å¸ƒå¼: å¯ç”¨ (World Size: {self.config.distributed.world_size})")
            if self.config.distributed.get("num_runs", 1) > 1:
                print(f"   å¤šè½®è¿è¡Œ: {self.config.distributed.num_runs} è½®")
        else:
            print(f"   åˆ†å¸ƒå¼: ç¦ç”¨")
        
        if self.config.debug.enabled:
            print(f"   è°ƒè¯•æ¨¡å¼: å¯ç”¨ (æ–‡ä»¶: {self.config.debug.max_files}, é¡¹ç›®: {self.config.debug.max_items_per_file})")


def load_task_manager(task_config_path: str) -> TaskManager:
    """åŠ è½½ä»»åŠ¡ç®¡ç†å™¨"""
    return TaskManager(task_config_path)
