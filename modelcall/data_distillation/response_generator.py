"""å“åº”ç”Ÿæˆå™¨ - æ•°æ®è’¸é¦ç¬¬ä¸‰æ­¥ï¼Œä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆå“åº”"""

import asyncio
import json
import logging
import os
import random
import sys
import copy
import traceback
import datetime
import time
import hashlib
from pathlib import Path
from typing import Dict, Any, List, Optional

import aiofiles
import jsonlines
from tqdm import tqdm

from ..common.model_client import UnifiedModelClient

# ä¼˜å…ˆå°è¯•ä½¿ç”¨ uvloop æå‡äº‹ä»¶å¾ªç¯æ€§èƒ½ï¼ˆè‹¥ä¸å¯ç”¨åˆ™å¿½ç•¥ï¼‰
try:
    import uvloop  # type: ignore
    asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
except Exception:
    pass

class ResponseGenerator:
    """å¼‚æ­¥å“åº”ç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        input_path: str,
        output_path: str,
        client_config: Dict[str, Any],
        chat_config: Dict[str, Any],
        concurrency: int = 20,
        batch_size: int = 20,
        flush_interval_secs: float = 2.0,
        retry_mode: bool = False,
        resume_mode: bool = True
    ):
        """
        åˆå§‹åŒ–å“åº”ç”Ÿæˆå™¨
        
        Args:
            input_path: è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºç›®å½•è·¯å¾„
            client_config: OpenAIå®¢æˆ·ç«¯é…ç½®
            chat_config: èŠå¤©é…ç½®
            concurrency: å¹¶å‘æ•°é‡
            batch_size: æ‰¹é‡ä¿å­˜å¤§å°
            flush_interval_secs: å®šæ—¶åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰
            retry_mode: æ˜¯å¦ä¸ºé‡è¯•æ¨¡å¼ï¼ˆé‡æ–°å¤„ç†å¤±è´¥çš„ä»»åŠ¡ï¼‰
            resume_mode: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆè·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡ï¼‰
        """
        self.input_path = input_path
        self.output_path = output_path
        self.concurrency = concurrency
        self.batch_size = batch_size
        self.flush_interval_secs = flush_interval_secs
        self.retry_mode = retry_mode
        self.resume_mode = resume_mode
        
        # é…ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # åˆ›å»ºç»Ÿä¸€æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆé…ç½®å·²ä¼ å…¥ï¼Œä¸å†éœ€è¦ä¿å­˜ï¼‰
        unified_config = {
            "client_config": client_config,
            "chat_config": chat_config
        }
        self.model_client = UnifiedModelClient(
            config=unified_config,
            max_concurrent_requests=concurrency,
            timeout=client_config.get("timeout", 600),
            max_retries=client_config.get("max_retries", 3)
        )
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.ensure_directory_exists(output_path, type="dir")
        
        # å†™æ–‡ä»¶äº’æ–¥é”ï¼Œé¿å…å¹¶å‘å†™å…¥äº¤é”™
        self.file_lock = asyncio.Lock()
        
        # å¼‚æ­¥å†™å…¥é˜Ÿåˆ—ï¼ˆåœ¨ run() ä¸­åˆå§‹åŒ–å¹¶å¯åŠ¨ writer ä»»åŠ¡ï¼‰
        self.writer_queue = None
        self.writer_task = None
    
    @staticmethod
    def ensure_directory_exists(path, type="file"):
        """ç¡®ä¿æŒ‡å®šè·¯å¾„çš„ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é€’å½’åˆ›å»º"""
        if not os.path.isabs(path):
            path = os.path.abspath(path)
        path = Path(path)
        if type == "file":
            parent_dir = path.parent
            os.makedirs(parent_dir, exist_ok=True)
        elif type == "dir":
            os.makedirs(path, exist_ok=True)
        else:
            raise ValueError(f"Invalid type: {type}")
    
    @staticmethod
    def read_jsonl_file(file_name, max_sentence=None):
        """è¯»å–JSONLæ–‡ä»¶"""
        data = []
        with jsonlines.open(file_name, "r") as r:
            for i, obj in enumerate(r):
                if max_sentence is not None and i >= max_sentence:
                    break
                data.append(obj)
        return data
    
    @staticmethod
    def write_jsonl_file(objs, path, chunk_size=1000, format="w"):
        """åŒæ­¥å†™å…¥JSONLæ–‡ä»¶"""
        ResponseGenerator.ensure_directory_exists(path, type="file")
        with jsonlines.open(path, format, flush=True) as w:
            for i in range(0, len(objs), chunk_size):
                w.write_all(objs[i: i + chunk_size])
    
    @staticmethod
    async def write_jsonl_file_async(objs, path, chunk_size=100, format="w"):
        """å¼‚æ­¥å†™å…¥JSONLæ–‡ä»¶ - ä¼˜åŒ–ç‰ˆ"""
        ResponseGenerator.ensure_directory_exists(path, type="file")
        mode = 'w' if format == 'w' else 'a'
        async with aiofiles.open(path, mode, encoding='utf-8') as f:
            for i in range(0, len(objs), chunk_size):
                chunk = objs[i: i + chunk_size]
                # âš¡ ä¼˜åŒ–ï¼šæ‰¹é‡åºåˆ—åŒ–åä¸€æ¬¡æ€§å†™å…¥ï¼ˆå‡å°‘ I/O è°ƒç”¨ 99%ï¼‰
                lines = '\n'.join(json.dumps(obj, ensure_ascii=False) for obj in chunk)
                await f.write(lines + '\n')
            await f.flush()
    
    @staticmethod
    def count_lines_in_file(file_path):
        """è®¡ç®—æ–‡ä»¶è¡Œæ•°"""
        try:
            with open(file_path, "r", encoding='utf-8') as f:
                return sum(1 for _ in f)
        except FileNotFoundError:
            return 0
    
    @staticmethod
    def ensure_uid(obj: Dict[str, Any]) -> str:
        """
        ç¡®ä¿å¯¹è±¡æœ‰å”¯ä¸€IDï¼Œå¦‚æœæ²¡æœ‰åˆ™åŸºäºå†…å®¹ç”Ÿæˆç¨³å®šçš„ UID
        
        Args:
            obj: æ•°æ®å¯¹è±¡
            
        Returns:
            uid: å¯¹è±¡çš„å”¯ä¸€æ ‡è¯†ç¬¦
        """
        # ä¼˜å…ˆä½¿ç”¨å·²æœ‰çš„ uid æˆ– id
        if 'uid' in obj:
            return obj['uid']
        if 'id' in obj:
            return obj['id']
        
        # å¦‚æœæ²¡æœ‰ uidï¼ŒåŸºäºå†…å®¹ç”Ÿæˆç¨³å®šçš„ UID
        # ä½¿ç”¨ messages çš„ç¬¬ä¸€æ¡å†…å®¹ï¼ˆæœ€ç¨³å®šï¼‰
        if 'messages' in obj and isinstance(obj['messages'], list) and len(obj['messages']) > 0:
            content = obj['messages'][0].get('content', '')
            if content:
                uid = hashlib.md5(content.encode('utf-8')).hexdigest()
                obj['uid'] = uid  # æ·»åŠ åˆ°å¯¹è±¡ä¸­
                return uid
        
        # æœ€åå…œåº•ï¼šåŸºäºæ•´ä¸ªå¯¹è±¡ç”Ÿæˆï¼ˆæ’åºé”®ä»¥ä¿è¯ç¨³å®šæ€§ï¼‰
        content_str = json.dumps(obj, sort_keys=True, ensure_ascii=False)
        uid = hashlib.md5(content_str.encode('utf-8')).hexdigest()
        obj['uid'] = uid
        return uid
    
    @staticmethod
    def deduplicate_by_uid(existing_objs, new_objs):
        """æ ¹æ®uidå»é‡ï¼Œè¿”å›å»é‡åçš„æ–°å¯¹è±¡åˆ—è¡¨"""
        existing_uids = set()
        
        # æ”¶é›†ç°æœ‰å¯¹è±¡çš„uid
        for obj in existing_objs:
            if 'uid' in obj:
                existing_uids.add(obj['uid'])
            elif 'id' in obj:
                existing_uids.add(obj['id'])
        
        # è¿‡æ»¤æ–°å¯¹è±¡
        deduplicated = []
        for obj in new_objs:
            obj_uid = obj.get('uid', obj.get('id', None))
            if obj_uid is None or obj_uid not in existing_uids:
                deduplicated.append(obj)
            # å¦‚æœæœ‰uidå°±æ·»åŠ åˆ°å·²å­˜åœ¨é›†åˆä¸­
            if obj_uid is not None:
                existing_uids.add(obj_uid)
        
        return deduplicated
    
    async def _chat_async(self, messages: List[Dict[str, str]]) -> str:
        """å¼‚æ­¥è°ƒç”¨ç»Ÿä¸€æ¨¡å‹å®¢æˆ·ç«¯"""
        # ä½¿ç”¨ç»Ÿä¸€å®¢æˆ·ç«¯çš„èŠå¤©è¡¥å…¨æ–¹æ³•ï¼ˆå¸¦é‡è¯•ï¼‰
        return await self.model_client.chat_completion(messages)
    
    async def task_worker_async(self, task_args):
        """å¼‚æ­¥ä»»åŠ¡å·¥ä½œå™¨
        
        å¹¶å‘æ§åˆ¶ç”± UnifiedModelClient å†…éƒ¨ç®¡ç†ã€‚
        """
        try:
            obj = task_args.get("obj", {})
            
            if "messages" not in obj:
                raise ValueError("Object missing 'messages' field")

            # âš¡ ä¼˜åŒ–ï¼šä½¿ç”¨æµ…æ‹·è´ä»£æ›¿æ·±æ‹·è´ï¼ˆæ€§èƒ½æå‡ 10-50 å€ï¼‰
            raw_text = obj["messages"][0]["content"]
            messages = [{"role": "user", "content": raw_text}]
            
            # ä½¿ç”¨ç»Ÿä¸€å®¢æˆ·ç«¯è¿›è¡Œè°ƒç”¨ï¼ˆå†…ç½®è¶…æ—¶å’Œé‡è¯•æœºåˆ¶ï¼‰
            response = await self._chat_async(messages)
            
            # æ„å»ºç»“æœï¼ˆæµ…æ‹·è´ + æ–°å­—æ®µï¼‰
            result = obj.copy()  # æµ…æ‹·è´è¶³å¤Ÿï¼ŒåŸå§‹æ•°æ®ä¸ä¼šè¢«ä¿®æ”¹
            result["response"] = response
            result["final_messages"] = [
                {
                    "role": "user",
                    "content": raw_text
                },
                {
                    "role": "assistant",
                    "content": response
                }
            ]
            return result
        except Exception as e:
            obj_id = task_args.get("obj", {}).get('uid', task_args.get("obj", {}).get('id', 'unknown'))
            error_detail = {
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc(),
                "timestamp": datetime.datetime.now().isoformat()
            }
            raise Exception(f"Task failed for object {obj_id}: {error_detail}")
    
    async def save_results_batch(self, output_objs, error_objs, output_path, error_path):
        """æ‰¹é‡ä¿å­˜ç»“æœï¼ˆä¸²è¡Œ+åŠ é”ï¼Œé¿å…å¹¶å‘å†™å…¥äº¤é”™ï¼‰"""
        async with self.file_lock:
            if output_objs:
                await self.write_jsonl_file_async(output_objs, output_path, format="a")
            if error_objs:
                await self.write_jsonl_file_async(error_objs, error_path, format="a")

    async def writer_loop(self, output_path: str, error_path: str):
        """åå°å†™å…¥åç¨‹ï¼šä¸²è¡Œæ¶ˆè´¹é˜Ÿåˆ—ï¼Œé¿å…é˜»å¡ä¸»è°ƒåº¦å¾ªç¯"""
        while True:
            item = await self.writer_queue.get()
            try:
                if item is None or item == (None, None):
                    # ç»ˆæ­¢ä¿¡å·
                    return
                output_objs, error_objs = item
                await self.save_results_batch(output_objs, error_objs, output_path, error_path)
            finally:
                # æ ‡è®°è¯¥é¡¹å¤„ç†å®Œæˆ
                self.writer_queue.task_done()
    
    def handle_retry_mode(self, input_path, output_path, retry_iter):
        """å¤„ç†é‡è¯•æ¨¡å¼çš„æ–‡ä»¶è·¯å¾„"""
        # æ„å»ºæ­£ç¡®çš„erroræ–‡ä»¶è·¯å¾„
        base_filename = os.path.basename(input_path)
        error_filename = base_filename.replace('.jsonl', '_error.jsonl')
        error_input_path = os.path.join(output_path, error_filename)
        
        if not os.path.exists(error_input_path):
            self.logger.error(f"Error file not found for retry: {error_input_path}")
            return None, None, None
        
        # ç§»åŠ¨åŸæ¥çš„erroræ–‡ä»¶
        retry_error_filename = base_filename.replace('.jsonl', f'_error_retry_{retry_iter}.jsonl')
        retry_error_path = os.path.join(output_path, retry_error_filename)
        os.rename(error_input_path, retry_error_path)
        self.logger.info(f"Moved previous error file: {error_input_path} -> {retry_error_path}")
        
        # è¯»å–ç°æœ‰çš„æˆåŠŸç»“æœç”¨äºå»é‡
        existing_success_objs = []
        success_output_path = os.path.join(output_path, base_filename)
        if os.path.exists(success_output_path):
            existing_success_objs = self.read_jsonl_file(success_output_path)
            self.logger.info(f"Found {len(existing_success_objs)} existing successful results")

        # ç§»åŠ¨åŸæ¥æˆåŠŸçš„æ–‡ä»¶
        success_filename = base_filename.replace('.jsonl', f'_success_retry_{retry_iter}.jsonl')
        success_path = os.path.join(output_path, success_filename)
        os.rename(success_output_path, success_path)
        self.logger.info(f"Moved previous success file: {success_output_path} -> {success_path}")
        
        return error_input_path, existing_success_objs, success_path
    
    async def run(self):
        """è¿è¡Œå“åº”ç”Ÿæˆä»»åŠ¡ï¼ˆä¼˜åŒ–ç‰ˆï¼šåŠ¨æ€ä»»åŠ¡æ± ï¼‰"""
        # å¤„ç†é‡è¯•æ¨¡å¼
        existing_success_objs = []
        retry_iter = 1
        
        if self.retry_mode:
            base_filename = os.path.basename(self.input_path)
            # æŸ¥æ‰¾é‡è¯•è½®æ•°
            while True:
                retry_error_filename = base_filename.replace('.jsonl', f'_error_retry_{retry_iter}.jsonl')
                retry_error_path = os.path.join(self.output_path, retry_error_filename)
                if not os.path.exists(retry_error_path):
                    break
                retry_iter += 1
            
            retry_result = self.handle_retry_mode(self.input_path, self.output_path, retry_iter)
            if retry_result[0] is None:
                return
            
            # é‡è¯•æ¨¡å¼ä¸‹ï¼Œå®é™…è¯»å–çš„æ˜¯erroræ–‡ä»¶ï¼Œä½†å·²ç»è¢«é‡å‘½åäº†
            retry_error_filename = os.path.basename(self.input_path).replace('.jsonl', f'_error_retry_{retry_iter}.jsonl')
            input_file_path = os.path.join(self.output_path, retry_error_filename)
            existing_success_objs = retry_result[1]
            self.logger.info(f"Retry mode: iteration {retry_iter}, reading from {input_file_path}")
        else:
            input_file_path = self.input_path
        
        # è¯»å–è¾“å…¥æ•°æ®
        if not os.path.exists(input_file_path):
            raise FileNotFoundError(f"Input file not found: {input_file_path}")
        
        objs = self.read_jsonl_file(input_file_path)
        self.logger.info(f"Loaded {len(objs)} objects from {input_file_path}")

        if not objs:
            self.logger.info("No objects to process")
            return

        # å‡†å¤‡ä»»åŠ¡é˜Ÿåˆ—ï¼Œå¹¶ç¡®ä¿æ¯ä¸ªå¯¹è±¡éƒ½æœ‰å”¯ä¸€ UID
        # âš¡ ä¼˜åŒ–ï¼šUID åªè®¡ç®—ä¸€æ¬¡å¹¶ç¼“å­˜åˆ° task å­—å…¸ä¸­ï¼Œé¿å…é‡å¤è®¡ç®—
        task_queue = []
        uid_missing_count = 0
        for obj in objs:
            original_has_uid = 'uid' in obj or 'id' in obj
            if not original_has_uid:
                uid_missing_count += 1
            # ç¡®ä¿æœ‰ UIDï¼ˆå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰å¹¶ç¼“å­˜
            uid = self.ensure_uid(obj)
            task_queue.append({"obj": obj, "uid": uid})  # ç¼“å­˜ UIDï¼Œé¿å…é‡å¤è®¡ç®—
        
        if uid_missing_count > 0:
            self.logger.info(f"ğŸ“‹ è‡ªåŠ¨ä¸º {uid_missing_count} ä¸ªä»»åŠ¡ç”Ÿæˆäº†ç¨³å®š UID")
        
        # è®¾ç½®è¾“å‡ºè·¯å¾„
        output_objs_path = os.path.join(self.output_path, os.path.basename(self.input_path))
        error_objs_path = os.path.join(self.output_path, os.path.basename(self.input_path).replace(".jsonl", "_error.jsonl"))
        self.logger.info(f"Output path: {output_objs_path}")
        self.logger.info(f"Error path: {error_objs_path}")
        
        # ============ æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡ ============
        if self.resume_mode and not self.retry_mode and os.path.exists(output_objs_path):
            self.logger.info("ğŸ”„ æ£€æµ‹åˆ°æ–­ç‚¹ç»­ä¼ æ¨¡å¼ï¼Œæ­£åœ¨åŠ è½½å·²å®Œæˆä»»åŠ¡...")
            
            # è¯»å–å·²å®Œæˆä»»åŠ¡çš„ UID
            completed_uids = set()
            try:
                with jsonlines.open(output_objs_path, mode='r') as reader:
                    for obj in reader:
                        # âœ… ä¼˜å…ˆä½¿ç”¨ç°æœ‰ uidï¼Œé¿å…å› ä¸ºè¿½åŠ çš„ response/final_messages å½±å“å“ˆå¸Œ
                        uid = obj.get('uid')
                        if not uid:
                            uid = self.ensure_uid(obj)
                        completed_uids.add(uid)
                
                if completed_uids:
                    self.logger.info(f"ğŸ“‹ å·²å®Œæˆä»»åŠ¡æ•°é‡: {len(completed_uids)}")
                    
                    # âš¡ ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜çš„ UIDï¼Œæ— éœ€é‡æ–°è®¡ç®—ï¼ˆæ€§èƒ½æå‡ 10-100 å€ï¼‰
                    original_count = len(task_queue)
                    task_queue = [
                        task for task in task_queue
                        if task['uid'] not in completed_uids  # ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„ UID
                    ]
                    
                    skipped_count = original_count - len(task_queue)
                    self.logger.info(f"âœ… è·³è¿‡ {skipped_count} ä¸ªå·²å®Œæˆä»»åŠ¡")
                    self.logger.info(f"ğŸ“ å‰©ä½™ {len(task_queue)} ä¸ªä»»åŠ¡éœ€è¦å¤„ç†")
                    
                    if not task_queue:
                        self.logger.info("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œæ— éœ€ç»§ç»­å¤„ç†")
                        return
                else:
                    self.logger.info("âš ï¸  è¾“å‡ºæ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©ºï¼Œä»å¤´å¼€å§‹")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸  è¯»å–å·²å®Œæˆä»»åŠ¡æ—¶å‡ºé”™: {e}ï¼Œå°†ä»å¤´å¼€å§‹")
        
        # æ‰“ä¹±ä»»åŠ¡é¡ºåºï¼ˆåœ¨è¿‡æ»¤åï¼‰
        random.shuffle(task_queue)
        
        # åœ¨é‡è¯•æ¨¡å¼ä¸‹å‡†å¤‡å»é‡é›†åˆ
        existing_uids_set = set()
        if self.retry_mode and existing_success_objs:
            for obj in existing_success_objs:
                uid = obj.get('uid', obj.get('id', None))
                if uid is not None:
                    existing_uids_set.add(uid)
        
        # æ‰¹é‡ç¼“å†²åŒº
        buffer_output = []
        buffer_errors = []
        last_flush_time = time.monotonic()
        
        # è¿›åº¦è·Ÿè¸ªå’Œæ€§èƒ½ç»Ÿè®¡
        total_tasks = len(task_queue)
        completed_count = 0
        success_count = 0
        error_count = 0
        start_time = time.monotonic()
        update_counter = 0  # âš¡ ä¼˜åŒ–ï¼šæ‰¹é‡æ›´æ–°è¿›åº¦æ¡çš„è®¡æ•°å™¨
        update_interval = 10  # æ¯10ä¸ªä»»åŠ¡æ›´æ–°ä¸€æ¬¡è¿›åº¦æ¡
        
        # åˆ›å»ºè¿›åº¦æ¡ï¼ˆåŠ¨æ€æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼‰
        progress_bar = tqdm(
            total=total_tasks,
            desc="Processing",
            file=sys.stdout,
            ncols=120,
            leave=True,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
        )
        
        # å¯åŠ¨åå°å†™å…¥ä»»åŠ¡ä¸é˜Ÿåˆ—
        self.writer_queue = asyncio.Queue(maxsize=10)
        self.writer_task = asyncio.create_task(self.writer_loop(output_objs_path, error_objs_path))

        # ============ æ ¸å¿ƒä¼˜åŒ–ï¼šåŠ¨æ€ä»»åŠ¡æ±  ============
        # ä»»åŠ¡æ± å¤§å° = concurrency * 2ï¼ˆå¹³è¡¡å†…å­˜å’Œæ•ˆç‡ï¼‰
        pool_size = self.concurrency * 2
        task_index = 0  # å½“å‰å¾…åˆ›å»ºä»»åŠ¡çš„ç´¢å¼•
        pending_tasks = set()  # é£è¡Œä¸­çš„ä»»åŠ¡é›†åˆ
        
        # åˆå§‹å¡«å……ä»»åŠ¡æ± ï¼ˆä¿å­˜ä»»åŠ¡ç´¢å¼•ä»¥ä¾¿é”™è¯¯è¿½è¸ªï¼‰
        task_to_index = {}  # ä»»åŠ¡ -> ç´¢å¼•æ˜ å°„
        while task_index < min(pool_size, total_tasks):
            task = asyncio.create_task(self.task_worker_async(task_queue[task_index]))
            pending_tasks.add(task)
            task_to_index[task] = task_index
            task_index += 1
        
        # åŠ¨æ€å¤„ç†ä»»åŠ¡å®Œæˆå’Œè¡¥å……
        while pending_tasks:
            # ç­‰å¾…ä»»æ„ä¸€ä¸ªä»»åŠ¡å®Œæˆ
            done, pending_tasks = await asyncio.wait(
                pending_tasks, 
                return_when=asyncio.FIRST_COMPLETED
            )
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for task in done:
                task_idx = task_to_index.pop(task, -1)  # è·å–å¹¶ç§»é™¤ä»»åŠ¡ç´¢å¼•
                
                try:
                    result = task.result()  # âš¡ ä¼˜åŒ–ï¼šç›´æ¥è·å–ç»“æœï¼Œæ— éœ€ awaitï¼ˆä»»åŠ¡å·²å®Œæˆï¼‰
                    
                    # é‡è¯•æ¨¡å¼ä¸‹çš„å»é‡
                    if self.retry_mode:
                        uid = result.get('uid', result.get('id', None))
                        if uid is not None and uid in existing_uids_set:
                            # è·³è¿‡é‡å¤ï¼ˆä½†è®¡å…¥å®Œæˆæ•°ï¼‰
                            pass
                        else:
                            if uid is not None:
                                existing_uids_set.add(uid)
                            buffer_output.append(result)
                            success_count += 1
                    else:
                        buffer_output.append(result)
                        success_count += 1
                        
                except Exception as e:
                    # å®Œæ•´é”™è¯¯å¤„ç†ï¼šä¿ç•™åŸå§‹ä»»åŠ¡æ•°æ®
                    error_count += 1
                    original_task = task_queue[task_idx] if task_idx >= 0 else {}
                    # âš¡ ä¼˜åŒ–ï¼šä½¿ç”¨æµ…æ‹·è´ï¼ˆé”™è¯¯å¯¹è±¡ä¸éœ€è¦æ·±æ‹·è´ï¼‰
                    error_obj = original_task.get("obj", {}).copy()
                    error_obj["error"] = str(e)
                    error_obj["error_type"] = type(e).__name__
                    error_obj["traceback"] = traceback.format_exc()
                    error_obj["timestamp"] = datetime.datetime.now().isoformat()
                    error_obj["task_index"] = task_idx
                    buffer_errors.append(error_obj)
                
                completed_count += 1
                update_counter += 1
                
                # âš¡ ä¼˜åŒ–ï¼šæ‰¹é‡æ›´æ–°è¿›åº¦æ¡ï¼ˆå‡å°‘å¼€é”€ï¼‰
                if update_counter >= update_interval:
                    elapsed_time = time.monotonic() - start_time
                    rate = completed_count / elapsed_time if elapsed_time > 0 else 0
                    progress_bar.set_description(
                        f"âœ… {success_count} | âŒ {error_count} | {rate:.1f} tasks/s"
                    )
                    progress_bar.update(update_counter)
                    update_counter = 0
                
                # ç«‹å³è¡¥å……æ–°ä»»åŠ¡ï¼ˆå¦‚æœè¿˜æœ‰ï¼‰
                if task_index < total_tasks:
                    new_task = asyncio.create_task(self.task_worker_async(task_queue[task_index]))
                    pending_tasks.add(new_task)
                    task_to_index[new_task] = task_index
                    task_index += 1
            
            # ä¼˜åŒ–çš„æ‰¹é‡åˆ·æ–°ï¼šåˆ†ç¦»æ‰¹é‡è§¦å‘å’Œå®šæ—¶è§¦å‘ï¼ˆæ”¹ä¸ºåå°é˜Ÿåˆ—å†™å…¥ï¼Œéé˜»å¡ï¼‰
            buffer_size = len(buffer_output) + len(buffer_errors)
            
            # ä¼˜å…ˆæ£€æŸ¥æ‰¹é‡å¤§å°ï¼ˆé¿å…ä¸å¿…è¦çš„æ—¶é—´æ£€æŸ¥ï¼‰
            if buffer_size >= self.batch_size:
                await self.writer_queue.put((buffer_output.copy(), buffer_errors.copy()))
                buffer_output.clear()
                buffer_errors.clear()
                last_flush_time = time.monotonic()
            elif buffer_size > 0:
                # åªæœ‰åœ¨æœ‰æ•°æ®æ—¶æ‰æ£€æŸ¥æ—¶é—´
                current_time = time.monotonic()
                if current_time - last_flush_time >= self.flush_interval_secs:
                    await self.writer_queue.put((buffer_output.copy(), buffer_errors.copy()))
                    buffer_output.clear()
                    buffer_errors.clear()
                    last_flush_time = current_time
        
        # âš¡ ä¼˜åŒ–ï¼šæ›´æ–°å‰©ä½™çš„è¿›åº¦
        if update_counter > 0:
            progress_bar.update(update_counter)
        
        # æœ€ç»ˆåˆ·æ–°ï¼šå°†æ®‹ä½™ç¼“å†²æ¨é€è‡³é˜Ÿåˆ—
        if buffer_output or buffer_errors:
            await self.writer_queue.put((buffer_output.copy(), buffer_errors.copy()))
            buffer_output.clear()
            buffer_errors.clear()
        
        # ç­‰å¾…é˜Ÿåˆ—å¤„ç†å®Œæ‰€æœ‰å†™å…¥ï¼Œå†ä¼˜é›…å…³é—­ writer
        await self.writer_queue.join()
        await self.writer_queue.put((None, None))
        await self.writer_task

        # å…³é—­è¿›åº¦æ¡
        progress_bar.close()
        
        # ============ Retry æ¨¡å¼ï¼šè‡ªåŠ¨åˆå¹¶å†å²æˆåŠŸç»“æœ ============
        if self.retry_mode and existing_success_objs:
            self.logger.info(f"\nğŸ”„ Retry æ¨¡å¼ï¼šæ­£åœ¨åˆå¹¶å†å²æˆåŠŸç»“æœ...")
            
            # è¯»å–å½“å‰ retry æ–°ç”Ÿæˆçš„ç»“æœ
            current_success = []
            if os.path.exists(output_objs_path):
                current_success = self.read_jsonl_file(output_objs_path)
            
            # åˆå¹¶ï¼šå†å²æˆåŠŸ + retry æ–°æˆåŠŸ
            merged_count = len(existing_success_objs) + len(current_success)
            self.logger.info(f"   å†å²æˆåŠŸ: {len(existing_success_objs)} æ¡")
            self.logger.info(f"   Retry æ–°æˆåŠŸ: {len(current_success)} æ¡")
            self.logger.info(f"   åˆå¹¶æ€»æ•°: {merged_count} æ¡")
            
            # å†™å…¥åˆå¹¶åçš„ç»“æœï¼ˆè¦†ç›–æ¨¡å¼ï¼‰
            all_success = existing_success_objs + current_success
            self.write_jsonl_file(all_success, output_objs_path, chunk_size=1000, format="w")
            
            self.logger.info(f"âœ… å·²è‡ªåŠ¨åˆå¹¶åˆ°: {output_objs_path}")
        
        # ç»Ÿè®¡æœ€ç»ˆç»“æœå’Œæ€§èƒ½æŒ‡æ ‡
        total_time = time.monotonic() - start_time
        avg_rate = completed_count / total_time if total_time > 0 else 0
        success_rate = (success_count / completed_count * 100) if completed_count > 0 else 0
        
        len_output_objs = self.count_lines_in_file(output_objs_path)
        len_error_objs = self.count_lines_in_file(error_objs_path)
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ“Š ä»»åŠ¡å®Œæˆç»Ÿè®¡")
        self.logger.info(f"{'='*60}")
        if self.retry_mode:
            self.logger.info(f"ğŸ”„ Retry æœ¬è½®æˆåŠŸ: {success_count} æ¡")
            self.logger.info(f"ğŸ”„ Retry æœ¬è½®å¤±è´¥: {error_count} æ¡")
            self.logger.info(f"ğŸ“ æœ€ç»ˆåˆå¹¶æ€»æ•°: {len_output_objs} æ¡")
        else:
            self.logger.info(f"âœ… æˆåŠŸ: {success_count} æ¡")
            self.logger.info(f"âŒ å¤±è´¥: {error_count} æ¡")
        self.logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f}s")
        self.logger.info(f"ğŸš€ å¹³å‡é€Ÿç‡: {avg_rate:.1f} tasks/s")
        self.logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_objs_path} ({len_output_objs} æ¡)")
        self.logger.info(f"ğŸ“ é”™è¯¯æ–‡ä»¶: {error_objs_path} ({len_error_objs} æ¡)")
        self.logger.info(f"{'='*60}")

