"""ç»Ÿä¸€çš„æ¨¡å‹å®¢æˆ·ç«¯ - æ”¯æŒæ•°æ®è¯„åˆ†å’Œæ•°æ®è’¸é¦çš„APIè°ƒç”¨"""

import os
import asyncio
import json
from typing import Dict, Any, List, Optional, Union
from pathlib import Path

import yaml
from openai import AsyncOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential


class UnifiedModelClient:
    """ç»Ÿä¸€çš„å¼‚æ­¥æ¨¡å‹å®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§é…ç½®æ–¹å¼"""
    
    def __init__(
        self,
        config: Optional[Dict[str, Any]] = None,
        config_path: Optional[str] = None,
        use_env: bool = False,
        max_concurrent_requests: int = 10,
        timeout: int = 600,
        max_retries: int = 3
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯
        
        Args:
            config: ç›´æ¥ä¼ å…¥çš„é…ç½®å­—å…¸ï¼Œæ ¼å¼ï¼š{"client_config": {...}, "chat_config": {...}}
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆYAMLæˆ–JSONï¼‰
            use_env: æ˜¯å¦ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆBASE_URL, API_KEYï¼‰
            max_concurrent_requests: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.max_concurrent_requests = max_concurrent_requests
        self.default_timeout = timeout
        self.default_max_retries = max_retries
        
        # åŠ è½½é…ç½®
        if config:
            self.client_config = config.get("client_config", {})
            self.chat_config = config.get("chat_config", {})
        elif config_path:
            loaded_config = self._load_config_file(config_path)
            self.client_config = loaded_config.get("client_config", {})
            self.chat_config = loaded_config.get("chat_config", {})
        elif use_env:
            self.client_config = {
                "base_url": os.environ.get("BASE_URL", "https://api.openai.com/v1"),
                "api_key": os.environ.get("API_KEY", ""),
                "timeout": int(os.environ.get("TIMEOUT", str(timeout))),
                "max_retries": int(os.environ.get("MAX_RETRIES", str(max_retries)))
            }
            self.chat_config = {
                "model": os.environ.get("MODEL_NAME", "gpt-3.5-turbo"),
                "temperature": float(os.environ.get("TEMPERATURE", "0.7")),
                "max_tokens": int(os.environ.get("MAX_TOKENS", "4000"))
            }
        else:
            raise ValueError("å¿…é¡»æä¾› configã€config_path æˆ– use_env ä¹‹ä¸€")
        
        # éªŒè¯å¿…éœ€é…ç½®
        if not self.client_config.get("api_key"):
            raise ValueError("API key is required")
        if not self.chat_config.get("model"):
            raise ValueError("Model name is required")
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = self._init_client()
        
        # å¹¶å‘æ§åˆ¶
        self.semaphore = asyncio.Semaphore(max_concurrent_requests)
    
    def _load_config_file(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆæ”¯æŒYAMLå’ŒJSONï¼‰"""
        config_path = Path(config_path)
        
        with open(config_path, 'r', encoding='utf-8') as f:
            if config_path.suffix in ['.yaml', '.yml']:
                return yaml.safe_load(f)
            elif config_path.suffix == '.json':
                return json.load(f)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼: {config_path.suffix}")
    
    def _init_client(self) -> AsyncOpenAI:
        """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯"""
        return AsyncOpenAI(
            base_url=self.client_config.get("base_url", "https://api.openai.com/v1"),
            api_key=self.client_config["api_key"],
            timeout=self.client_config.get("timeout", self.default_timeout),
            max_retries=self.client_config.get("max_retries", self.default_max_retries)
        )
    
    async def _chat_completion_raw(
        self, 
        messages: List[Dict[str, str]],
        **kwargs
    ) -> str:
        """åŸå§‹çš„èŠå¤©è¡¥å…¨è¯·æ±‚ï¼ˆæ— é‡è¯•ï¼‰"""
        async with self.semaphore:
            # åˆå¹¶é…ç½®å‚æ•°
            chat_params = {**self.chat_config, **kwargs}
            chat_params["messages"] = messages
            
            # ç§»é™¤ä¸æ˜¯ API å‚æ•°çš„é…ç½®
            timeout = chat_params.pop("timeout", self.client_config.get("timeout", self.default_timeout))
            # å…¼å®¹ reasoning å‚æ•°ï¼š
            # - OpenAI Responses: reasoning = {effort: ...}
            # - GPT-OSS: reasoning_effort = "low|medium|high"
            # é€šè¿‡ extra_body ä¼ é€’ï¼Œé¿å… SDK å‚æ•°æ ¡éªŒæŠ¥é”™
            extra_body = {}
            if "reasoning" in chat_params:
                extra_body["reasoning"] = chat_params.pop("reasoning")
            if "reasoning_effort" in chat_params:
                extra_body["reasoning_effort"] = chat_params.pop("reasoning_effort")
            
            if extra_body:
                response = await self.client.chat.completions.create(**chat_params, extra_body=extra_body)
            else:
                response = await self.client.chat.completions.create(**chat_params)
            
            # å¤„ç†å“åº”å†…å®¹
            content = ""
            reasoning_content = ""
            
            # å¤„ç†æµå¼å“åº”
            if chat_params.get("stream", False):
                async for chunk in response:
                    if hasattr(chunk, "choices") and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if hasattr(delta, "content") and delta.content:
                            content += delta.content
                        # å…¼å®¹å­—æ®µï¼šreasoning_content æˆ– reasoningï¼ˆGPT-OSSï¼‰
                        if hasattr(delta, "reasoning_content") and getattr(delta, "reasoning_content"):
                            reasoning_content += delta.reasoning_content
                        elif hasattr(delta, "reasoning") and getattr(delta, "reasoning"):
                            # reasoning å¯èƒ½æ˜¯å­—ç¬¦ä¸²å¢é‡ï¼Œè¿™é‡Œç›´æ¥æ‹¼æ¥
                            reasoning_content += delta.reasoning
            else:
                # éæµå¼å“åº”
                if hasattr(response, "choices") and len(response.choices) > 0:
                    message = response.choices[0].message
                    # åŒæ—¶è¯»å– content ä¸ reasoning/reasoning_contentï¼ˆä¸èƒ½ç”¨ elifï¼‰
                    if hasattr(message, "content") and message.content:
                        content = message.content
                    if hasattr(message, "reasoning_content") and message.reasoning_content:
                        reasoning_content = message.reasoning_content
                    # GPT-OSS è¿”å› message.reasoning
                    if hasattr(message, "reasoning") and message.reasoning and not reasoning_content:
                        reasoning_content = message.reasoning
            
            # ç»„åˆæ€ç»´é“¾å’Œå†…å®¹
            if reasoning_content and reasoning_content.strip():
                return f"<think>\n{reasoning_content}\n</think>\n\n{content}"
            else:
                return content if content else reasoning_content
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=5, min=4, max=60))
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> str:
        """
        èŠå¤©è¡¥å…¨è¯·æ±‚ï¼ˆå¸¦é‡è¯•ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: é¢å¤–çš„èŠå¤©å‚æ•°ï¼ˆä¼šè¦†ç›–é»˜è®¤é…ç½®ï¼‰
        
        Returns:
            str: æ¨¡å‹å“åº”å†…å®¹
        """
        try:
            return await self._chat_completion_raw(messages, **kwargs)
        except Exception as e:
            print(f"ğŸ”„ APIè°ƒç”¨å¤±è´¥ï¼Œé‡è¯•ä¸­: {type(e).__name__} - {str(e)}")
            raise
    
    async def batch_chat_completion(
        self,
        messages_list: List[List[Dict[str, str]]],
        **kwargs
    ) -> List[str]:
        """
        æ‰¹é‡èŠå¤©è¡¥å…¨è¯·æ±‚
        
        Args:
            messages_list: æ¶ˆæ¯åˆ—è¡¨çš„åˆ—è¡¨
            **kwargs: é¢å¤–çš„èŠå¤©å‚æ•°
        
        Returns:
            List[str]: æ¨¡å‹å“åº”å†…å®¹åˆ—è¡¨
        """
        tasks = [
            self.chat_completion(messages, **kwargs)
            for messages in messages_list
        ]
        return await asyncio.gather(*tasks)
    
    def get_chat_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰çš„èŠå¤©é…ç½®"""
        return self.chat_config.copy()
    
    def get_client_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰çš„å®¢æˆ·ç«¯é…ç½®ï¼ˆéšè—APIå¯†é’¥ï¼‰"""
        config = self.client_config.copy()
        if "api_key" in config:
            config["api_key"] = "***" + config["api_key"][-4:] if len(config["api_key"]) > 4 else "***"
        return config


class ModelClientFactory:
    """æ¨¡å‹å®¢æˆ·ç«¯å·¥å‚ç±»"""
    
    @staticmethod
    def from_config_file(config_path: str, **kwargs) -> UnifiedModelClient:
        """ä»é…ç½®æ–‡ä»¶åˆ›å»ºå®¢æˆ·ç«¯"""
        return UnifiedModelClient(config_path=config_path, **kwargs)
    
    @staticmethod
    def from_config_dict(config: Dict[str, Any], **kwargs) -> UnifiedModelClient:
        """ä»é…ç½®å­—å…¸åˆ›å»ºå®¢æˆ·ç«¯"""
        return UnifiedModelClient(config=config, **kwargs)
    
    @staticmethod
    def from_env(**kwargs) -> UnifiedModelClient:
        """ä»ç¯å¢ƒå˜é‡åˆ›å»ºå®¢æˆ·ç«¯"""
        return UnifiedModelClient(use_env=True, **kwargs)
    
    @staticmethod
    def from_task_config(task_config: Dict[str, Any], **kwargs) -> UnifiedModelClient:
        """
        ä»ä»»åŠ¡é…ç½®åˆ›å»ºå®¢æˆ·ç«¯
        
        Args:
            task_config: ä»»åŠ¡é…ç½®å­—å…¸ï¼Œå¯ä»¥åŒ…å«ï¼š
                - model_config_path: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
                - model_config: æ¨¡å‹é…ç½®å­—å…¸
                - æˆ–ç›´æ¥åŒ…å« client_config å’Œ chat_config
        """
        # ä¼˜å…ˆçº§ï¼šç›´æ¥é…ç½® > model_config > model_config_path > ç¯å¢ƒå˜é‡
        if "client_config" in task_config and "chat_config" in task_config:
            return UnifiedModelClient(config=task_config, **kwargs)
        elif "model_config" in task_config:
            return UnifiedModelClient(config=task_config["model_config"], **kwargs)
        elif "model_config_path" in task_config:
            return UnifiedModelClient(config_path=task_config["model_config_path"], **kwargs)
        else:
            # å›é€€åˆ°ç¯å¢ƒå˜é‡
            return UnifiedModelClient(use_env=True, **kwargs)


def save_model_config(config: Dict[str, Any], output_path: str) -> None:
    """
    ä¿å­˜æ¨¡å‹é…ç½®åˆ°æ–‡ä»¶
    
    Args:
        config: é…ç½®å­—å…¸
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆ.yaml æˆ– .jsonï¼‰
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        if output_path.suffix in ['.yaml', '.yml']:
            yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
        elif output_path.suffix == '.json':
            json.dump(config, f, ensure_ascii=False, indent=2)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {output_path.suffix}")
    
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {output_path}")

